# Batch Testing Infrastructure

## Overview

The batch testing infrastructure provides automated performance and memory leak detection for the document evaluation system through concurrent load testing and stress testing.

## Architecture

### Test Components

1. **Batch Load Test** (`test_batch_load.py`)
   - Tests 10 concurrent document evaluation tasks
   - Measures total execution time and throughput
   - Validates throughput >= 170 documents/hour (600ms per document average)
   - Generates `processing_times.json` and `throughput_report.json`

2. **Stress Test** (`test_stress_test.py`)
   - Tests 50 concurrent document uploads
   - Monitors worker memory usage over time using `tracemalloc` and `psutil`
   - Fails if memory growth exceeds 20% between start and end states
   - Detects memory leaks through monotonic growth pattern analysis
   - Generates `memory_profile.json`

### CI/CD Integration

The batch testing infrastructure is integrated into `.github/workflows/ci.yml` with two new jobs:

#### 1. `batch_load_test` Job

```yaml
batch_load_test:
  runs-on: ubuntu-latest
  needs: performance  # Depends on performance job completion
  services:
    redis:  # Required for Celery worker simulation
      image: redis:7-alpine
      ports:
        - 6379:6379
```

**Features:**
- Spawns 10 concurrent document evaluation tasks
- Measures throughput and validates against 170 docs/hour threshold
- Archives metrics as GitHub Actions artifacts
- Comments PR with performance results
- Runs after the `performance` job passes

#### 2. `stress_test` Job

```yaml
stress_test:
  runs-on: ubuntu-latest
  needs: performance  # Depends on performance job completion
  services:
    redis:  # Required for Celery worker simulation
      image: redis:7-alpine
      ports:
        - 6379:6379
```

**Features:**
- Spawns 50 concurrent upload tasks in batches
- Monitors memory usage after each batch
- Validates memory growth <= 20%
- Detects monotonic memory growth patterns (leaks)
- Archives metrics as GitHub Actions artifacts
- Comments PR with memory profile results
- Runs after the `performance` job passes

## Output Files

### 1. `processing_times.json`

Generated by batch load test. Contains:

```json
{
  "test_type": "batch_load_10_concurrent",
  "num_documents": 10,
  "total_time_seconds": 1.234,
  "avg_time_per_doc_ms": 123.4,
  "throughput_docs_per_hour": 291.5,
  "threshold_docs_per_hour": 170,
  "threshold_ms_per_doc": 600,
  "results": [...]
}
```

### 2. `throughput_report.json`

Generated by batch load test. Contains:

```json
{
  "test_name": "batch_load_test",
  "timestamp": 1234567890.123,
  "metrics": {
    "throughput": 291.5,
    "threshold": 170,
    "passed": true
  },
  "performance_summary": {
    "avg_ms_per_doc": 123.4,
    "total_time_seconds": 1.234,
    "documents_processed": 10
  }
}
```

### 3. `memory_profile.json`

Generated by stress test. Contains:

```json
{
  "test_type": "stress_test_50_concurrent",
  "num_documents": 50,
  "total_time_seconds": 2.567,
  "memory_stats": {
    "initial_memory_mb": 120.5,
    "final_memory_mb": 135.2,
    "memory_growth_mb": 14.7,
    "memory_growth_percent": 12.2,
    "threshold_percent": 20
  },
  "memory_samples_over_time": [...],
  "top_memory_allocations": [...]
}
```

## GitHub Actions Artifacts

All metrics are archived as GitHub Actions artifacts:

1. **batch-metrics** artifact:
   - `processing_times.json`
   - `throughput_report.json`

2. **stress-test-metrics** artifact:
   - `memory_profile.json`

Artifacts are available for download from the Actions tab in GitHub.

## Usage

### Running Tests Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Run batch load test
pytest test_batch_load.py -v

# Run stress test
pytest test_stress_test.py -v

# Validate test structure
python3 validate_batch_tests.py
```

### CI/CD Execution

Tests automatically run on:
- Push to `main` or `master` branches
- Pull requests

Both jobs run after the `performance` job completes successfully.

## Performance Requirements

### Batch Load Test
- **Throughput**: >= 170 documents/hour
- **Average Time**: <= 600ms per document
- **Concurrency**: 10 simultaneous requests

### Stress Test
- **Memory Growth**: <= 20% increase
- **Concurrency**: 50 simultaneous requests
- **Batch Size**: 10 documents per batch
- **Leak Detection**: Fails on monotonic memory growth

## Memory Profiling

The stress test uses two memory profiling approaches:

1. **psutil** - Process-level memory tracking
   - Measures RSS (Resident Set Size)
   - Samples memory after each batch
   - Tracks overall memory growth

2. **tracemalloc** - Python-level memory tracking
   - Tracks Python object allocations
   - Identifies top memory consumers
   - Provides detailed allocation traces

## Mock Implementation

Both tests use mock implementations for testing:

- `MockFastAPIClient` - Simulates FastAPI upload endpoint
- `MockCeleryWorker` - Simulates Celery worker with memory tracking

This allows tests to run without requiring actual FastAPI/Celery services, while still validating the test infrastructure and CI/CD pipeline.

## Integration with Deployment Infrastructure

The batch testing infrastructure complements the existing deployment infrastructure:

- **Canary Deployment** (`canary_deployment.py`) - Progressive traffic routing with automated rollback
- **OpenTelemetry Tracing** (`opentelemetry_instrumentation.py`) - Distributed tracing for 28 critical flows
- **SLO Monitoring** (`slo_monitoring.py`) - Real-time monitoring with 99.5% availability target

Together, these systems provide comprehensive performance validation, memory leak detection, and production monitoring.

## Validation

Use `validate_batch_tests.py` to verify the test infrastructure:

```bash
python3 validate_batch_tests.py
```

Validates:
- Python syntax correctness
- Required imports present
- Expected functions implemented
- Output file generation configured
- CI/CD jobs properly configured
- Artifact archival setup
- Redis service configuration

## References

- `.github/workflows/ci.yml` - CI/CD pipeline configuration
- `test_batch_load.py` - Batch load test implementation
- `test_stress_test.py` - Stress test with memory profiling
- `validate_batch_tests.py` - Validation script
- `DEPLOYMENT_INFRASTRUCTURE.md` - Deployment infrastructure documentation
