name: CI

on:
  push:
    branches: ["main", "master"]
  pull_request:

jobs:
  tests:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify Python bytecode compilation
        run: python -m compileall -q .

      - name: Lint with flake8
        run: flake8 .

      - name: Run pytest
        run: pytest -q
  
  performance:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run performance suite (100 iterations per component)
        id: perf_test
        run: |
          echo "Running performance benchmark suite..."
          python3 performance_test_suite.py > performance_output.txt 2>&1 || echo "PERF_FAILED=true" >> $GITHUB_ENV
          cat performance_output.txt

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report
          path: |
            performance_report.json
            performance_output.txt

      - name: Check performance budgets
        run: |
          if [ "$PERF_FAILED" == "true" ]; then
            echo "❌ Performance budgets exceeded - blocking PR"
            echo "Review performance_report.json for details"
            exit 1
          else
            echo "✅ All performance budgets met"
          fi

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const summary = report.summary;
              const results = report.results;
              
              let comment = '## 🚀 Performance Test Results\n\n';
              comment += `- **Total Components**: ${summary.total_components}\n`;
              comment += `- **Passed**: ${summary.passed} ✅\n`;
              comment += `- **Failed**: ${summary.failed} ❌\n\n`;
              
              comment += '### Component Performance\n\n';
              comment += '| Component | p95 Latency | Budget | Status |\n';
              comment += '|-----------|-------------|--------|--------|\n';
              
              for (const [name, result] of Object.entries(results)) {
                const status = result.budget_passed ? '✅' : '❌';
                comment += `| ${name} | ${result.p95_ms.toFixed(2)}ms | ${result.budget_message} | ${status} |\n`;
              }
              
              if (summary.failed > 0) {
                comment += '\n⚠️ **Performance budgets exceeded - PR blocked**\n';
              }
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not read performance report:', error);
            }
  
  soak_test:
    runs-on: ubuntu-latest
    needs: performance
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'run-soak-test')
    timeout-minutes: 300
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run 4-hour soak test
        id: soak_test
        run: |
          echo "Starting 4-hour soak test for memory leak detection..."
          python3 performance_test_suite.py --soak > soak_test_output.txt 2>&1 || echo "SOAK_FAILED=true" >> $GITHUB_ENV
          cat soak_test_output.txt

      - name: Upload soak test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: soak-test-results
          path: |
            performance_report.json
            soak_test_output.txt

      - name: Check for memory leaks
        run: |
          if [ "$SOAK_FAILED" == "true" ]; then
            echo "❌ Memory leak detected in 4-hour soak test - blocking PR"
            exit 1
          else
            echo "✅ No memory leaks detected"
          fi

      - name: Comment PR with soak test results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const soak = report.soak_test;
              
              if (soak) {
                let comment = '## 🔬 4-Hour Soak Test Results\n\n';
                comment += `- **Duration**: ${soak.duration_hours} hours\n`;
                comment += `- **Iterations**: ${soak.iterations}\n`;
                comment += `- **Memory Growth**: ${soak.memory_growth_mb_per_hour.toFixed(2)} MB/hour\n`;
                comment += `- **Initial Memory**: ${soak.initial_memory_mb.toFixed(2)} MB\n`;
                comment += `- **Final Memory**: ${soak.final_memory_mb.toFixed(2)} MB\n\n`;
                
                if (soak.leak_detected) {
                  comment += '❌ **Memory leak detected - PR blocked**\n';
                } else {
                  comment += '✅ **No memory leaks detected**\n';
                }
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              }
            } catch (error) {
              console.log('Could not read soak test report:', error);
            }

  freeze_verification:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify pipeline configuration freeze
        run: |
          echo "Verifying pipeline configuration is locked..."
          python3 determinism_guard.py --diagnostics
          python3 determinism_guard.py --seed 42 --strict

  evaluation:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run evaluation pipeline
        run: |
          echo "Running evaluation pipeline..."
          python3 run_evaluation.py --output artifacts/

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: evaluation-artifacts
          path: |
            artifacts/answers_report.json
            artifacts/*.json

  pre_execution_validation:
    runs-on: ubuntu-latest
    needs: freeze_verification
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run pre-execution validation
        run: |
          echo "Running pre-execution system validation..."
          python3 system_validators.py --pre

  triple_run_reproducibility:
    runs-on: ubuntu-latest
    needs: pre_execution_validation
    timeout-minutes: 60
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Create artifacts directory
        run: mkdir -p artifacts

      - name: Execute evaluation pipeline - Run 1
        run: |
          echo "Executing evaluation pipeline - Run 1..."
          python3 unified_evaluation_pipeline.py test_plan.pdf || echo "RUN1_FAILED=true" >> $GITHUB_ENV
          if [ -f flow_runtime.json ]; then
            mkdir -p artifacts
            cp flow_runtime.json artifacts/flow_runtime_run1.json
          fi

      - name: Execute evaluation pipeline - Run 2
        run: |
          echo "Executing evaluation pipeline - Run 2..."
          python3 unified_evaluation_pipeline.py test_plan.pdf || echo "RUN2_FAILED=true" >> $GITHUB_ENV
          if [ -f flow_runtime.json ]; then
            cp flow_runtime.json artifacts/flow_runtime_run2.json
          fi

      - name: Execute evaluation pipeline - Run 3
        run: |
          echo "Executing evaluation pipeline - Run 3..."
          python3 unified_evaluation_pipeline.py test_plan.pdf || echo "RUN3_FAILED=true" >> $GITHUB_ENV
          if [ -f flow_runtime.json ]; then
            cp flow_runtime.json artifacts/flow_runtime_run3.json
          fi

      - name: Verify deterministic ordering consistency
        run: |
          echo "Verifying deterministic ordering across three runs..."
          python3 -c "
import json
import sys
from pathlib import Path

runs = ['run1', 'run2', 'run3']
flow_files = [Path(f'artifacts/flow_runtime_{run}.json') for run in runs]

# Check all files exist
missing = [f for f in flow_files if not f.exists()]
if missing:
    print(f'ERROR: Missing flow_runtime.json files: {missing}')
    sys.exit(1)

# Load and compare orders
orders = []
for flow_file in flow_files:
    data = json.loads(flow_file.read_text())
    order = data.get('order', [])
    orders.append(order)
    print(f'{flow_file.name}: {len(order)} steps')

# Check all orders are identical
if orders[0] == orders[1] == orders[2]:
    print('✅ Deterministic ordering verified across all three runs')
    sys.exit(0)
else:
    print('❌ Ordering mismatch detected between runs')
    for i, order in enumerate(orders):
        print(f'Run {i+1} order: {order[:5]}...')
    sys.exit(1)
          "

      - name: Upload triple-run artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: triple-run-reproducibility
          path: |
            artifacts/flow_runtime_run1.json
            artifacts/flow_runtime_run2.json
            artifacts/flow_runtime_run3.json

  post_execution_validation:
    runs-on: ubuntu-latest
    needs: triple_run_reproducibility
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Download triple-run artifacts
        uses: actions/download-artifact@v4
        with:
          name: triple-run-reproducibility
          path: artifacts/

      - name: Run post-execution validation
        run: |
          echo "Running post-execution system validation..."
          python3 system_validators.py --post --artifacts artifacts

      - name: Verify 300/300 coverage
        run: |
          echo "Verifying 300/300 coverage in coverage_report.json..."
          python3 -c "
import json
import sys
from pathlib import Path

coverage_file = Path('artifacts/coverage_report.json')

if not coverage_file.exists():
    print('ERROR: coverage_report.json not found')
    sys.exit(1)

data = json.loads(coverage_file.read_text())
total_coverage = data.get('summary', {}).get('total_coverage', 0)

print(f'Total coverage: {total_coverage}/300')

if total_coverage >= 300:
    print('✅ Coverage verification passed: 300/300')
    sys.exit(0)
else:
    print(f'❌ Coverage verification failed: {total_coverage}/300')
    sys.exit(1)
          "

      - name: Verify flow_runtime.json determinism
        run: |
          echo "Verifying flow_runtime.json determinism..."
          python3 -c "
import json
import sys
from pathlib import Path

runtime_file = Path('artifacts/flow_runtime_run3.json')

if not runtime_file.exists():
    print('ERROR: flow_runtime.json not found')
    sys.exit(1)

data = json.loads(runtime_file.read_text())
order = data.get('order', [])

if len(order) > 0:
    print(f'✅ flow_runtime.json contains deterministic order: {len(order)} steps')
    sys.exit(0)
else:
    print('❌ flow_runtime.json has empty or missing order')
    sys.exit(1)
          "

      - name: Verify 1:1 question-to-rubric correspondence
        run: |
          echo "Verifying 1:1 question-to-rubric weight correspondence..."
          python3 rubric_check.py --answers artifacts/answers_report.json --rubric RUBRIC_SCORING.json || exit 3

      - name: Archive evaluation artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: evaluation-artifacts
          path: |
            artifacts/run*.json
            artifacts/answers_report.json
            artifacts/answers_sample.json
            artifacts/flow_runtime*.json
            artifacts/coverage_report.json
            artifacts/evidence_registry.json
            artifacts/module_to_questions_matrix.csv
            artifacts/nonrepudiation_bundle.zip
  
  rubric-validation:
    runs-on: ubuntu-latest
    needs: evaluation
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Download evaluation artifacts
        uses: actions/download-artifact@v4
        with:
          name: evaluation-artifacts
          path: artifacts/

      - name: Run rubric check tool
        id: rubric_check
        run: |
          echo "Running rubric validation..."
          python3 rubric_check.py \
            --answers artifacts/answers_report.json \
            --rubric rubric_scoring.json \
            --output-dir artifacts/ > rubric_check_output.txt 2>&1
          RUBRIC_EXIT_CODE=$?
          cat rubric_check_output.txt
          
          if [ $RUBRIC_EXIT_CODE -eq 2 ]; then
            echo "❌ Missing input files - failing pipeline"
            exit 2
          elif [ $RUBRIC_EXIT_CODE -eq 3 ]; then
            echo "❌ Question-weight mismatch detected - failing pipeline"
            exit 3
          elif [ $RUBRIC_EXIT_CODE -eq 0 ]; then
            echo "✅ Rubric validation passed"
            exit 0
          else
            echo "❌ Rubric check failed with exit code $RUBRIC_EXIT_CODE"
            exit 1
          fi

      - name: Upload rubric validation artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rubric-validation-report
          path: |
            rubric_check_output.txt
            artifacts/rubric_weight_diff.txt
          retention-days: 30

      - name: Comment PR with rubric validation results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## 📋 Rubric Validation Results\n\n';
            
            try {
              const output = fs.readFileSync('rubric_check_output.txt', 'utf8');
              const exitCode = '${{ steps.rubric_check.outcome }}';
              
              if (exitCode === 'success') {
                comment += '✅ **All question weights validated successfully**\n\n';
                comment += '```\n' + output + '\n```\n';
              } else {
                comment += '❌ **Rubric validation failed**\n\n';
                comment += '```\n' + output + '\n```\n\n';
                
                try {
                  const diffContent = fs.readFileSync('artifacts/rubric_weight_diff.txt', 'utf8');
                  comment += '### Detailed Diff Report\n\n';
                  comment += '```\n' + diffContent + '\n```\n';
                } catch (err) {
                  comment += '⚠️ Could not read diff file\n';
                }
                
                comment += '\n**Action Required**: Fix question-weight mismatches before merging.\n';
              }
            } catch (error) {
              comment += '⚠️ **Could not parse validation output**\n\n';
              comment += `Error: ${error.message}\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
>>>>>>> 045ee61 (Add rubric validation job to CI workflow that checks answer reports against scoring rubric and uploads artifacts)
